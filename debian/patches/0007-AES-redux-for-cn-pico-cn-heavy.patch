From: Jason Rhinelander <jason@imaginary.ca>
Date: Thu, 9 Jul 2020 19:04:28 -0300
Subject: AES redux for cn-pico & cn-heavy

Force aes instruction availability in the pico & heavy implementations;
they each have runtime checks that will ensure they are supported before
called, and so this allows creating a binary that supports both non-AES
and AES CPUs.

This also unifies some of the common cpuid checking code, and moves it
to a single global variable (previously pico used a somewhat inefficient
static local variable, and cn-heavy used a hugely inefficiently cpuid
call on every hash call).

Also removes the x86-specific __m128i code from cn_heavy_hash.hpp and
adds a simple wrapper for it in the intel-specific code instead.
---
 src/crypto/cn_heavy_hash.hpp            | 101 +++-------------------
 src/crypto/cn_heavy_hash_hard_arm.cpp   |  17 +++-
 src/crypto/cn_heavy_hash_hard_intel.cpp | 140 ++++++++++++++++++------------
 src/crypto/cn_heavy_hash_soft.cpp       |  10 ++-
 src/crypto/cn_turtle_hash.c             | 145 ++++++++++----------------------
 5 files changed, 169 insertions(+), 244 deletions(-)

diff --git a/src/crypto/cn_heavy_hash.hpp b/src/crypto/cn_heavy_hash.hpp
index 49176fa..a5557db 100644
--- a/src/crypto/cn_heavy_hash.hpp
+++ b/src/crypto/cn_heavy_hash.hpp
@@ -31,80 +31,27 @@
 
 #pragma once
 
-#include <inttypes.h>
-#include <stddef.h>
-#include <stdlib.h>
-#include <assert.h>
-#include <string.h>
+#include <cinttypes>
+#include <cstddef>
+#include <cstdlib>
+#include <cassert>
+#include <cstring>
 #include <boost/align/aligned_alloc.hpp>
 
-#if defined(_WIN32) || defined(_WIN64)
-#include <malloc.h>
-#include <intrin.h>
-#define HAS_WIN_INTRIN_API
-#endif
-
-// Note HAS_INTEL_HW and HAS_ARM_HW only mean we can emit the AES instructions
-// check CPU support for the hardware AES encryption has to be done at runtime
 #if defined(__x86_64__) || defined(__i386__) || defined(_M_X86) || defined(_M_X64)
-#ifdef __GNUC__
-#include <x86intrin.h>
-#ifndef __clang__
-#pragma GCC target ("aes")
-#endif
-#if !defined(HAS_WIN_INTRIN_API)
-#include <cpuid.h>
-#endif // !defined(HAS_WIN_INTRIN_API)
-#endif // __GNUC__
-#define HAS_INTEL_HW
-#endif
-
-#if defined(__aarch64__)
-#ifndef __clang__
-#pragma GCC target ("+crypto")
-#endif
-#include <sys/auxv.h>
-#include <asm/hwcap.h>
-#include <arm_neon.h>
-#define HAS_ARM_HW
+#  define HAS_INTEL_HW
+#elif defined(__aarch64__)
+#  define HAS_ARM_HW
 #endif
 
-#ifdef HAS_INTEL_HW
-inline void cpuid(uint32_t eax, int32_t ecx, int32_t val[4])
+#if defined(HAS_INTEL_HW) || defined(HAS_ARM_HW)
+inline bool check_override()
 {
-	val[0] = 0;
-	val[1] = 0;
-	val[2] = 0;
-	val[3] = 0;
-
-#if defined(HAS_WIN_INTRIN_API)
-	__cpuidex(val, eax, ecx);
-#else
-	__cpuid_count(eax, ecx, val[0], val[1], val[2], val[3]);
-#endif
-}
-
-inline bool hw_check_aes()
-{
-	int32_t cpu_info[4];
-	cpuid(1, 0, cpu_info);
-	return (cpu_info[2] & (1 << 25)) != 0;
-}
-#endif
-
-#ifdef HAS_ARM_HW
-inline bool hw_check_aes()
-{
-	return (getauxval(AT_HWCAP) & HWCAP_AES) != 0;
-}
-#endif
-
-#if !defined(HAS_INTEL_HW) && !defined(HAS_ARM_HW)
-inline bool hw_check_aes()
-{
-	return false;
+	const char *env = getenv("LOKI_USE_SOFTWARE_AES");
+	return env && strcmp(env, "0") && strcmp(env, "no");
 }
 #endif
+extern "C" const bool cpu_aes_enabled;
 
 // This cruft avoids casting-galore and allows us not to worry about sizeof(void*)
 class cn_sptr
@@ -114,9 +61,6 @@ public:
 	cn_sptr(uint64_t* ptr) { base_ptr = ptr; }
 	cn_sptr(uint32_t* ptr) { base_ptr = ptr; }
 	cn_sptr(uint8_t* ptr) { base_ptr = ptr; }
-#ifdef HAS_INTEL_HW
-	cn_sptr(__m128i* ptr) { base_ptr = ptr; }
-#endif
 
 	inline void set(void* ptr) { base_ptr = ptr; }
 	inline cn_sptr offset(size_t i) { return reinterpret_cast<uint8_t*>(base_ptr)+i; }
@@ -133,9 +77,6 @@ public:
 	inline int32_t& as_dword(size_t i) { return *(reinterpret_cast<int32_t*>(base_ptr)+i); }
 	inline uint32_t& as_udword(size_t i) { return *(reinterpret_cast<uint32_t*>(base_ptr)+i); }
 	inline const uint32_t& as_udword(size_t i) const { return *(reinterpret_cast<uint32_t*>(base_ptr)+i); }
-#ifdef HAS_INTEL_HW
-	inline __m128i* as_xmm() { return reinterpret_cast<__m128i*>(base_ptr); }
-#endif
 private:
 	void* base_ptr;
 };
@@ -190,7 +131,7 @@ public:
 
 	void hash(const void* in, size_t len, void* out, bool prehashed=false)
 	{
-		if(hw_check_aes() && !check_override())
+		if(cpu_aes_enabled)
 			hardware_hash(in, len, out, prehashed);
 		else
 			software_hash(in, len, out, prehashed);
@@ -217,20 +158,6 @@ private:
 		borrowed_pad = true;
 	}
 
-	inline bool check_override()
-	{
-		const char *env = getenv("LOKI_USE_SOFTWARE_AES");
-		if (!env) {
-			return false;
-		}
-		else if (!strcmp(env, "0") || !strcmp(env, "no")) {
-			return false;
-		}
-		else {
-			return true;
-		}
-	}
-
 	inline void free_mem()
 	{
 		if(!borrowed_pad)
diff --git a/src/crypto/cn_heavy_hash_hard_arm.cpp b/src/crypto/cn_heavy_hash_hard_arm.cpp
index 8a28fde..050d524 100644
--- a/src/crypto/cn_heavy_hash_hard_arm.cpp
+++ b/src/crypto/cn_heavy_hash_hard_arm.cpp
@@ -29,12 +29,27 @@
 // Parts of this file are originally copyright (c) 2014-2017, The Monero Project
 // Parts of this file are originally copyright (c) 2012-2013, The Cryptonote developers
 
+#if defined(__aarch64__)
+
+#ifndef __clang__
+#  pragma GCC target ("+crypto")
+#endif
+
 #include "cn_heavy_hash.hpp"
 extern "C" {
 #include "../crypto/keccak.h"
 }
 
-#ifdef HAS_ARM_HW
+#include <sys/auxv.h>
+#include <asm/hwcap.h>
+#include <arm_neon.h>
+
+static bool hw_check_aes()
+{
+	return (getauxval(AT_HWCAP) & HWCAP_AES) != 0;
+}
+
+extern "C" const bool cpu_aes_enabled = hw_check_aes() && check_override();
 
 extern const uint8_t saes_sbox[256];
 
diff --git a/src/crypto/cn_heavy_hash_hard_intel.cpp b/src/crypto/cn_heavy_hash_hard_intel.cpp
index aa14ac4..6cdf94d 100644
--- a/src/crypto/cn_heavy_hash_hard_intel.cpp
+++ b/src/crypto/cn_heavy_hash_hard_intel.cpp
@@ -29,12 +29,43 @@
 // Parts of this file are originally copyright (c) 2014-2017, The Monero Project
 // Parts of this file are originally copyright (c) 2012-2013, The Cryptonote developers
 
+#if defined(__x86_64__) || defined(__i386__) || defined(_M_X86) || defined(_M_X64)
+
+#ifdef __GNUC__
+#  ifndef __clang__
+     // Force on aes support; we do a cpuid check at runtime before it actually gets invoked.
+#    pragma GCC target ("aes,sse2")
+#  endif
+#  include <x86intrin.h>
+#endif
+
 #include "cn_heavy_hash.hpp"
+
 extern "C" {
 #include "../crypto/keccak.h"
 }
 
-#ifdef HAS_INTEL_HW
+#if defined(_WIN32) || defined(_WIN64)
+#  include <malloc.h>
+#  include <intrin.h>
+#  define HAS_WIN_INTRIN_API
+#else
+#  include <cpuid.h>
+#endif
+
+static bool hw_check_aes()
+{
+	int32_t cpu_info[4] = {0};
+
+#if defined(HAS_WIN_INTRIN_API)
+	__cpuidex(cpu_info, 1, 0);
+#else
+	__cpuid_count(1, 0, cpu_info[0], cpu_info[1], cpu_info[2], cpu_info[3]);
+#endif
+	return (cpu_info[2] & (1 << 25)) != 0;
+}
+
+extern "C" const bool cpu_aes_enabled = hw_check_aes() && check_override();
 
 #if !defined(_LP64) && !defined(_WIN64)
 #define BUILD32
@@ -118,33 +149,36 @@ inline void xor_shift(__m128i& x0, __m128i& x1, __m128i& x2, __m128i& x3, __m128
     x7 = _mm_xor_si128(x7, tmp0);
 }
 
+static inline __m128i* as_xmm(cn_sptr& x) { return reinterpret_cast<__m128i*>(x.as_void()); }
+static inline __m128i* as_xmm(cn_sptr&& x) { return reinterpret_cast<__m128i*>(x.as_void()); }
+
 template<size_t MEMORY, size_t ITER, size_t VERSION>
 void cn_heavy_hash<MEMORY,ITER,VERSION>::implode_scratchpad_hard()
 {
 	__m128i x0, x1, x2, x3, x4, x5, x6, x7;
 	__m128i k0, k1, k2, k3, k4, k5, k6, k7, k8, k9;
 	
-	aes_genkey(spad.as_xmm() + 2, k0, k1, k2, k3, k4, k5, k6, k7, k8, k9);
+	aes_genkey(as_xmm(spad) + 2, k0, k1, k2, k3, k4, k5, k6, k7, k8, k9);
 
-	x0 = _mm_load_si128(spad.as_xmm() + 4);
-	x1 = _mm_load_si128(spad.as_xmm() + 5);
-	x2 = _mm_load_si128(spad.as_xmm() + 6);
-	x3 = _mm_load_si128(spad.as_xmm() + 7);
-	x4 = _mm_load_si128(spad.as_xmm() + 8);
-	x5 = _mm_load_si128(spad.as_xmm() + 9);
-	x6 = _mm_load_si128(spad.as_xmm() + 10);
-	x7 = _mm_load_si128(spad.as_xmm() + 11);
+	x0 = _mm_load_si128(as_xmm(spad) + 4);
+	x1 = _mm_load_si128(as_xmm(spad) + 5);
+	x2 = _mm_load_si128(as_xmm(spad) + 6);
+	x3 = _mm_load_si128(as_xmm(spad) + 7);
+	x4 = _mm_load_si128(as_xmm(spad) + 8);
+	x5 = _mm_load_si128(as_xmm(spad) + 9);
+	x6 = _mm_load_si128(as_xmm(spad) + 10);
+	x7 = _mm_load_si128(as_xmm(spad) + 11);
 
 	for (size_t i = 0; i < MEMORY / sizeof(__m128i); i +=8)
 	{
-		x0 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 0), x0);
-		x1 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 1), x1);
-		x2 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 2), x2);
-		x3 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 3), x3);
-		x4 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 4), x4);
-		x5 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 5), x5);
-		x6 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 6), x6);
-		x7 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 7), x7);
+		x0 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 0), x0);
+		x1 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 1), x1);
+		x2 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 2), x2);
+		x3 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 3), x3);
+		x4 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 4), x4);
+		x5 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 5), x5);
+		x6 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 6), x6);
+		x7 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 7), x7);
 
 		aes_round8(k0, x0, x1, x2, x3, x4, x5, x6, x7);
 		aes_round8(k1, x0, x1, x2, x3, x4, x5, x6, x7);
@@ -163,14 +197,14 @@ void cn_heavy_hash<MEMORY,ITER,VERSION>::implode_scratchpad_hard()
 
 	for (size_t i = 0; VERSION > 0 && i < MEMORY / sizeof(__m128i); i +=8)
 	{
-		x0 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 0), x0);
-		x1 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 1), x1);
-		x2 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 2), x2);
-		x3 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 3), x3);
-		x4 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 4), x4);
-		x5 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 5), x5);
-		x6 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 6), x6);
-		x7 = _mm_xor_si128(_mm_load_si128(lpad.as_xmm() + i + 7), x7);
+		x0 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 0), x0);
+		x1 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 1), x1);
+		x2 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 2), x2);
+		x3 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 3), x3);
+		x4 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 4), x4);
+		x5 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 5), x5);
+		x6 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 6), x6);
+		x7 = _mm_xor_si128(_mm_load_si128(as_xmm(lpad) + i + 7), x7);
 
 		aes_round8(k0, x0, x1, x2, x3, x4, x5, x6, x7);
 		aes_round8(k1, x0, x1, x2, x3, x4, x5, x6, x7);
@@ -202,14 +236,14 @@ void cn_heavy_hash<MEMORY,ITER,VERSION>::implode_scratchpad_hard()
 		xor_shift(x0, x1, x2, x3, x4, x5, x6, x7);
 	}
 
-	_mm_store_si128(spad.as_xmm() + 4, x0);
-	_mm_store_si128(spad.as_xmm() + 5, x1);
-	_mm_store_si128(spad.as_xmm() + 6, x2);
-	_mm_store_si128(spad.as_xmm() + 7, x3);
-	_mm_store_si128(spad.as_xmm() + 8, x4);
-	_mm_store_si128(spad.as_xmm() + 9, x5);
-	_mm_store_si128(spad.as_xmm() + 10, x6);
-	_mm_store_si128(spad.as_xmm() + 11, x7);
+	_mm_store_si128(as_xmm(spad) + 4, x0);
+	_mm_store_si128(as_xmm(spad) + 5, x1);
+	_mm_store_si128(as_xmm(spad) + 6, x2);
+	_mm_store_si128(as_xmm(spad) + 7, x3);
+	_mm_store_si128(as_xmm(spad) + 8, x4);
+	_mm_store_si128(as_xmm(spad) + 9, x5);
+	_mm_store_si128(as_xmm(spad) + 10, x6);
+	_mm_store_si128(as_xmm(spad) + 11, x7);
 }
 
 template<size_t MEMORY, size_t ITER, size_t VERSION>
@@ -218,16 +252,16 @@ void cn_heavy_hash<MEMORY,ITER,VERSION>::explode_scratchpad_hard()
 	__m128i x0, x1, x2, x3, x4, x5, x6, x7;
 	__m128i k0, k1, k2, k3, k4, k5, k6, k7, k8, k9;
 
-	aes_genkey(spad.as_xmm(), k0, k1, k2, k3, k4, k5, k6, k7, k8, k9);
+	aes_genkey(as_xmm(spad), k0, k1, k2, k3, k4, k5, k6, k7, k8, k9);
 
-	x0 = _mm_load_si128(spad.as_xmm() + 4);
-	x1 = _mm_load_si128(spad.as_xmm() + 5);
-	x2 = _mm_load_si128(spad.as_xmm() + 6);
-	x3 = _mm_load_si128(spad.as_xmm() + 7);
-	x4 = _mm_load_si128(spad.as_xmm() + 8);
-	x5 = _mm_load_si128(spad.as_xmm() + 9);
-	x6 = _mm_load_si128(spad.as_xmm() + 10);
-	x7 = _mm_load_si128(spad.as_xmm() + 11);
+	x0 = _mm_load_si128(as_xmm(spad) + 4);
+	x1 = _mm_load_si128(as_xmm(spad) + 5);
+	x2 = _mm_load_si128(as_xmm(spad) + 6);
+	x3 = _mm_load_si128(as_xmm(spad) + 7);
+	x4 = _mm_load_si128(as_xmm(spad) + 8);
+	x5 = _mm_load_si128(as_xmm(spad) + 9);
+	x6 = _mm_load_si128(as_xmm(spad) + 10);
+	x7 = _mm_load_si128(as_xmm(spad) + 11);
 
 	for (size_t i = 0; VERSION > 0 && i < 16; i++)
 	{
@@ -258,14 +292,14 @@ void cn_heavy_hash<MEMORY,ITER,VERSION>::explode_scratchpad_hard()
 		aes_round8(k8, x0, x1, x2, x3, x4, x5, x6, x7);
 		aes_round8(k9, x0, x1, x2, x3, x4, x5, x6, x7);
 
-		_mm_store_si128(lpad.as_xmm() + i + 0, x0);
-		_mm_store_si128(lpad.as_xmm() + i + 1, x1);
-		_mm_store_si128(lpad.as_xmm() + i + 2, x2);
-		_mm_store_si128(lpad.as_xmm() + i + 3, x3);
-		_mm_store_si128(lpad.as_xmm() + i + 4, x4);
-		_mm_store_si128(lpad.as_xmm() + i + 5, x5);
-		_mm_store_si128(lpad.as_xmm() + i + 6, x6);
-		_mm_store_si128(lpad.as_xmm() + i + 7, x7);
+		_mm_store_si128(as_xmm(lpad) + i + 0, x0);
+		_mm_store_si128(as_xmm(lpad) + i + 1, x1);
+		_mm_store_si128(as_xmm(lpad) + i + 2, x2);
+		_mm_store_si128(as_xmm(lpad) + i + 3, x3);
+		_mm_store_si128(as_xmm(lpad) + i + 4, x4);
+		_mm_store_si128(as_xmm(lpad) + i + 5, x5);
+		_mm_store_si128(as_xmm(lpad) + i + 6, x6);
+		_mm_store_si128(as_xmm(lpad) + i + 7, x7);
 	}
 }
 
@@ -343,11 +377,11 @@ void cn_heavy_hash<MEMORY,ITER,VERSION>::hardware_hash(const void* in, size_t le
 	for(size_t i = 0; i < ITER; i++)
 	{
 		__m128i cx;
-		cx = _mm_load_si128(scratchpad_ptr(idx0).as_xmm());
+		cx = _mm_load_si128(as_xmm(scratchpad_ptr(idx0)));
 
 		cx = _mm_aesenc_si128(cx, _mm_set_epi64x(ah0, al0));
 
-		_mm_store_si128(scratchpad_ptr(idx0).as_xmm(), _mm_xor_si128(bx0, cx));
+		_mm_store_si128(as_xmm(scratchpad_ptr(idx0)), _mm_xor_si128(bx0, cx));
 		idx0 = xmm_extract_64(cx);
 		bx0 = cx;
 
diff --git a/src/crypto/cn_heavy_hash_soft.cpp b/src/crypto/cn_heavy_hash_soft.cpp
index 142430f..43717ef 100644
--- a/src/crypto/cn_heavy_hash_soft.cpp
+++ b/src/crypto/cn_heavy_hash_soft.cpp
@@ -31,8 +31,16 @@
 #include "cn_heavy_hash.hpp"
 extern "C" {
 #include "../crypto/keccak.h"
+#if !defined(__clang__) && defined(HAS_INTEL_HW)
+#  include <x86intrin.h>
+#endif
 }
 
+
+#if !defined(HAS_INTEL_HW) && !defined(HAS_ARM_HW)
+extern "C" const bool cpu_aes_enabled = false;
+#endif
+
 /*
 AES Tables Implementation is
 ---------------------------------------------------------------------------
@@ -167,7 +175,7 @@ inline uint32_t sub_word(uint32_t key)
 		(saes_sbox[(key >> 8)  & 0xff] << 8  ) | saes_sbox[key & 0xff];
 }
 
-#if defined(__clang__) || !(defined(__x86_64__) || defined(__i386__))
+#if defined(__clang__) || !defined(HAS_INTEL_HW)
 inline uint32_t rotr(uint32_t value, uint32_t amount)
 {
 	return (value >> amount) | (value << ((32 - amount) & 31));
diff --git a/src/crypto/cn_turtle_hash.c b/src/crypto/cn_turtle_hash.c
index 1ab7f7c..e3d6c9a 100644
--- a/src/crypto/cn_turtle_hash.c
+++ b/src/crypto/cn_turtle_hash.c
@@ -22,6 +22,8 @@
 #define INIT_SIZE_BLK          8
 #define INIT_SIZE_BYTE         (INIT_SIZE_BLK * AES_BLOCK_SIZE)
 
+extern const bool cpu_aes_enabled;
+
 extern int aesb_single_round(const uint8_t *in, uint8_t*out, const uint8_t *expandedKey);
 extern int aesb_pseudo_round(const uint8_t *in, uint8_t *out, const uint8_t *expandedKey);
 
@@ -188,44 +190,38 @@ extern int aesb_pseudo_round(const uint8_t *in, uint8_t *out, const uint8_t *exp
     lo ^= *(U64(hp_state + (j ^ 0x20)) + 1); \
   } while (0)
 
-#if defined(__AES__) && (defined(__x86_64__) || (defined(_MSC_VER) && defined(_WIN64)))
-// Optimised code below, uses x86-specific intrinsics, SSE2, AES-NI
-// Fall back to more portable code is down at the bottom
 
-#include <emmintrin.h>
+#ifdef HAS_INTEL_HW // ARCH x86, x86-64
+// Optimised code below, uses x86-specific intrinsics, SSE2, AES-NI.  We do a cpuid runtime check
+// before actually calling any AES code, and otherwise fall back to more portable code.
 
-#if defined(_MSC_VER)
-#include <intrin.h>
-#include <windows.h>
-#define STATIC
-#define INLINE __inline
-#if !defined(RDATA_ALIGN16)
-#define RDATA_ALIGN16 __declspec(align(16))
-#endif
-#elif defined(__MINGW32__)
-#include <intrin.h>
-#include <windows.h>
-#define STATIC static
-#define INLINE inline
-#if !defined(RDATA_ALIGN16)
-#define RDATA_ALIGN16 __attribute__ ((aligned(16)))
-#endif
+#include <emmintrin.h>
+#if defined(_MSC_VER) || defined(__MINGW32__)
+#  include <intrin.h>
+#  include <windows.h>
 #else
-#include <wmmintrin.h>
-#include <sys/mman.h>
-#define STATIC static
-#define INLINE inline
-#if !defined(RDATA_ALIGN16)
-#define RDATA_ALIGN16 __attribute__ ((aligned(16)))
+#  include <wmmintrin.h>
+#  include <sys/mman.h>
 #endif
+
+#if defined(__GNUC__) && !defined(__clang__)
+#  pragma GCC target ("aes,sse2")
 #endif
 
-#if defined(__INTEL_COMPILER)
-#define ASM __asm__
-#elif !defined(_MSC_VER)
-#define ASM __asm__
+#if defined(_MSC_VER)
+#  define ASM __asm
+#  define STATIC
+#  define INLINE __inline
+#  if !defined(RDATA_ALIGN16)
+#    define RDATA_ALIGN16 __declspec(align(16))
+#  endif
 #else
-#define ASM __asm
+#  define ASM __asm__
+#  define STATIC static
+#  define INLINE inline
+#  if !defined(RDATA_ALIGN16)
+#    define RDATA_ALIGN16 __attribute__ ((aligned(16)))
+#  endif
 #endif
 
 #define U64(x) ((uint64_t *) (x))
@@ -233,17 +229,17 @@ extern int aesb_pseudo_round(const uint8_t *in, uint8_t *out, const uint8_t *exp
 
 #define state_index(x,div) (((*((uint64_t *)x) >> 4) & (TOTALBLOCKS /(div) - 1)) << 4)
 #if defined(_MSC_VER)
-#if !defined(_WIN64)
-#define __mul() lo = mul128(c[0], b[0], &hi);
-#else
-#define __mul() lo = _umul128(c[0], b[0], &hi);
-#endif
-#else
-#if defined(__x86_64__)
-#define __mul() ASM("mulq %3\n\t" : "=d"(hi), "=a"(lo) : "%a" (c[0]), "rm" (b[0]) : "cc");
+#  if !defined(_WIN64)
+#    define __mul() lo = mul128(c[0], b[0], &hi);
+#  else
+#    define __mul() lo = _umul128(c[0], b[0], &hi);
+#  endif
 #else
-#define __mul() lo = mul128(c[0], b[0], &hi);
-#endif
+#  if defined(__x86_64__)
+#    define __mul() ASM("mulq %3\n\t" : "=d"(hi), "=a"(lo) : "%a" (c[0]), "rm" (b[0]) : "cc");
+#  else
+#    define __mul() lo = mul128(c[0], b[0], &hi);
+#  endif
 #endif
 
 #define pre_aes() \
@@ -301,23 +297,6 @@ union cn_turtle_hash_state
 THREADV uint8_t *hp_state = NULL;
 THREADV int hp_allocated = 0;
 
-#if defined(_MSC_VER)
-#define cpuid(info,x)    __cpuidex(info,x,0)
-#else
-void cpuid(int CPUInfo[4], int InfoType)
-{
-  ASM __volatile__
-  (
-  "cpuid":
-    "=a" (CPUInfo[0]),
-    "=b" (CPUInfo[1]),
-    "=c" (CPUInfo[2]),
-    "=d" (CPUInfo[3]) :
-        "a" (InfoType), "c" (0)
-    );
-}
-#endif
-
 /**
  * @brief a = (a xor b), where a and b point to 128 bit values
  */
@@ -333,43 +312,6 @@ STATIC INLINE void xor64(uint64_t *a, const uint64_t b)
   *a ^= b;
 }
 
-/**
- * @brief uses cpuid to determine if the CPU supports the AES instructions
- * @return true if the CPU supports AES, false otherwise
- */
-
-STATIC INLINE int force_software_aes(void)
-{
-  static int use = -1;
-
-  if (use != -1)
-    return use;
-
-  const char *env = getenv("TURTLECOIN_USE_SOFTWARE_AES");
-  if (!env) {
-    use = 0;
-  }
-  else if (!strcmp(env, "0") || !strcmp(env, "no")) {
-    use = 0;
-  }
-  else {
-    use = 1;
-  }
-  return use;
-}
-
-STATIC INLINE int check_aes_hw(void)
-{
-  int cpuid_results[4];
-  static int supported = -1;
-
-  if(supported >= 0)
-    return supported;
-
-  cpuid(cpuid_results,1);
-  return supported = cpuid_results[2] & (1 << 25);
-}
-
 STATIC INLINE void aes_256_assist1(__m128i* t1, __m128i * t2)
 {
   __m128i t4;
@@ -684,7 +626,6 @@ void cn_turtle_hash(const void *data, size_t length, char *hash, int light, int
   size_t i, j;
   uint64_t *p = NULL;
   oaes_ctx *aes_ctx = NULL;
-  int useAes = !force_software_aes() && check_aes_hw();
 
   static void (*const extra_hashes[4])(const void *, size_t, char *) =
   {
@@ -708,7 +649,7 @@ void cn_turtle_hash(const void *data, size_t length, char *hash, int light, int
    * the 2MB large random access buffer.
    */
 
-  if(useAes)
+  if(cpu_aes_enabled)
   {
       aes_expand_key(state.hs.b, expandedKey);
       for(i = 0; i < init_rounds; i++)
@@ -743,8 +684,8 @@ void cn_turtle_hash(const void *data, size_t length, char *hash, int light, int
   _b = _mm_load_si128(R128(b));
   _b1 = _mm_load_si128(R128(b) + 1);
   // Two independent versions, one with AES, one without, to ensure that
-  // the useAes test is only performed once, not every iteration.
-  if(useAes)
+  // the cpu_aes_enabled test is only performed once, not every iteration.
+  if(cpu_aes_enabled)
   {
       for(i = 0; i < aes_rounds; i++)
       {
@@ -768,7 +709,7 @@ void cn_turtle_hash(const void *data, size_t length, char *hash, int light, int
    * was originally created with the output of Keccak1600. */
 
   memcpy(text, state.init, INIT_SIZE_BYTE);
-  if(useAes)
+  if(cpu_aes_enabled)
   {
       aes_expand_key(&state.hs.b[32], expandedKey);
       for(i = 0; i < init_rounds; i++)
@@ -804,7 +745,7 @@ void cn_turtle_hash(const void *data, size_t length, char *hash, int light, int
   slow_hash_free_state(CN_TURTLE_PAGE_SIZE);
 }
 
-#elif defined(__arm__) || defined(__aarch64__)
+#elif defined(__arm__) || defined(__aarch64__) // ARCH arm
 void slow_hash_allocate_state(void)
 {
   // Do nothing, this is just to maintain compatibility with the upgraded slow-hash.c
@@ -1373,7 +1314,7 @@ void cn_turtle_hash(const void *data, size_t length, char *hash, int light, int
 }
 #endif /* !aarch64 || !crypto */
 
-#else
+#else // ARCH fallback
 // Portable implementation as a fallback
 
 void slow_hash_allocate_state(void)
